{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, BaseRetriever\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import Field\n",
    "import re\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OpenAI API key not found. Set it in the .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Tesseract OCR path \n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\abhishekk\\OneDrive - Universitetet i Agder\\Welcome\\Language\\Langchain Project\\Rag_Chatbot\\data_4_case\\BMW_2021.pdf\n",
      "Processing: C:\\Users\\abhishekk\\OneDrive - Universitetet i Agder\\Welcome\\Language\\Langchain Project\\Rag_Chatbot\\data_4_case\\BMW_2022.pdf\n",
      "Processing: C:\\Users\\abhishekk\\OneDrive - Universitetet i Agder\\Welcome\\Language\\Langchain Project\\Rag_Chatbot\\data_4_case\\BMW_2023.pdf\n",
      "Processing: C:\\Users\\abhishekk\\OneDrive - Universitetet i Agder\\Welcome\\Language\\Langchain Project\\Rag_Chatbot\\data_4_case\\Ford_2021.pdf\n",
      "Processing: C:\\Users\\abhishekk\\OneDrive - Universitetet i Agder\\Welcome\\Language\\Langchain Project\\Rag_Chatbot\\data_4_case\\Ford_2022.pdf\n",
      "Processing: C:\\Users\\abhishekk\\OneDrive - Universitetet i Agder\\Welcome\\Language\\Langchain Project\\Rag_Chatbot\\data_4_case\\Ford_2023.pdf\n",
      "Processing: C:\\Users\\abhishekk\\OneDrive - Universitetet i Agder\\Welcome\\Language\\Langchain Project\\Rag_Chatbot\\data_4_case\\news.pdf\n",
      "Processing: C:\\Users\\abhishekk\\OneDrive - Universitetet i Agder\\Welcome\\Language\\Langchain Project\\Rag_Chatbot\\data_4_case\\Tesla_2022.pdf\n",
      "Processing: C:\\Users\\abhishekk\\OneDrive - Universitetet i Agder\\Welcome\\Language\\Langchain Project\\Rag_Chatbot\\data_4_case\\Tesla_2023.pdf\n",
      "Total chunks extracted: 5939\n"
     ]
    }
   ],
   "source": [
    "# Data processing functions\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract and split text from the PDF file into manageable chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "                                                         separators=[\"\\n\\n\", \"\\n\", \". \", \"Revenue\", \"Profit\", \"Expenses\", \"Net Income\", \"Table\", \"Figure\", \"Summary\"],  # Split by paragraphs, then sentences, then characters\n",
    "                                                        chunk_size=500,\n",
    "                                                        chunk_overlap=50\n",
    "                                                    )\n",
    "        chunks = text_splitter.split_documents(docs)\n",
    "        return [{\"type\": \"text\", \"content\": chunk.page_content, \"metadata\": chunk.metadata} for chunk in chunks]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract tables from PDF\n",
    "def extract_tables_with_pdfplumber(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract tables from a PDF using PDFPlumber and return them as structured chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table_chunks = []\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    df = pd.DataFrame(table)\n",
    "                    table_text = df.to_string(index=False, header=True)\n",
    "                    metadata = {\"source\": file_path, \"page\": page_num, \"type\": \"table\"}\n",
    "                    table_chunks.append({\"type\": \"table\", \"content\": table_text, \"metadata\": metadata})\n",
    "        return table_chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from {file_path}: {e}\")\n",
    "        return []\n",
    "# Function to extract text from images using OCR\n",
    "def extract_text_from_images(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract text from images in a PDF using Tesseract OCR.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(file_path)\n",
    "        image_chunks = []\n",
    "        for page_num, image in enumerate(images, start=1):\n",
    "            ocr_text = pytesseract.image_to_string(image).strip()\n",
    "            if ocr_text:\n",
    "                metadata = {\"source\": file_path, \"page\": page_num, \"type\": \"image\"}\n",
    "                image_chunks.append({\"type\": \"image\", \"content\": ocr_text, \"metadata\": metadata})\n",
    "        return image_chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from images in {file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "# Main function to process a single PDF\n",
    "def process_pdf(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a PDF to extract text, tables, and OCR-based content into structured chunks.\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    chunks = []\n",
    "    chunks.extend(extract_text_from_pdf(file_path))\n",
    "    chunks.extend(extract_tables_with_pdfplumber(file_path))\n",
    "    chunks.extend(extract_text_from_images(file_path))\n",
    "    return chunks\n",
    "\n",
    "# Function to process multiple PDFs in a directory\n",
    "def process_multiple_pdfs(directory: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process all PDF files in a directory and return embedding-ready chunks.\n",
    "    \"\"\"\n",
    "    files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".pdf\")]\n",
    "    all_chunks = []\n",
    "    for file_path in files:\n",
    "        all_chunks.extend(process_pdf(file_path))\n",
    "    return all_chunks\n",
    "\n",
    "# Configure file directory\n",
    "file_directory = r\"C:\\Users\\abhishekk\\OneDrive - Universitetet i Agder\\Welcome\\Language\\Langchain Project\\Rag_Chatbot\\data_4_case\"\n",
    "chunks = process_multiple_pdfs(file_directory)\n",
    "print(f\"Total chunks extracted: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save a FAISS vector store\n",
    "def create_faiss_vector_store(chunks: List[Dict[str, Any]], faiss_index_path: str = \"faiss_index\") -> FAISS:\n",
    "    \"\"\"\n",
    "    Create and save a FAISS vector store using OpenAI embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    contents = [chunk[\"content\"] for chunk in chunks]\n",
    "    metadatas = [chunk[\"metadata\"] for chunk in chunks]\n",
    "    faiss_store = FAISS.from_texts(contents, embeddings, metadatas=metadatas)\n",
    "    return faiss_store\n",
    "\n",
    "vector_store = create_faiss_vector_store(chunks, faiss_index_path=\"pdf_faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Classify Query\n",
    "def classify_query_with_year(query: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Classify the query as basic, comparison, or aggregation and extract year(s).\n",
    "    \"\"\"\n",
    "    years = re.findall(r\"\\b\\d{4}\\b\", query)\n",
    "    query_lower = query.lower()\n",
    "    comparison_keywords = [\"compare\", \"difference\", \"vs\", \"how does\", \"comparison between\"]\n",
    "    aggregation_keywords = [\"total\", \"sum\", \"aggregate\", \"combined\", \"overall\"]\n",
    "\n",
    "    if any(keyword in query_lower for keyword in comparison_keywords):\n",
    "        query_type = \"comparison\"\n",
    "    elif any(keyword in query_lower for keyword in aggregation_keywords):\n",
    "        query_type = \"aggregation\"\n",
    "    else:\n",
    "        query_type = \"basic\"\n",
    "\n",
    "    return query_type, years\n",
    "\n",
    "# Step 2: Helper: Entity Extraction for Comparisons\n",
    "def extract_entities_from_query(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract entities from a query for comparison tasks.\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    entities = []\n",
    "    if \"bmw\" in query_lower:\n",
    "        entities.append(\"BMW\")\n",
    "    if \"tesla\" in query_lower:\n",
    "        entities.append(\"Tesla\")\n",
    "    if \"ford\" in query_lower:\n",
    "        entities.append(\"Ford\")\n",
    "    return entities\n",
    "# Step 3: Retrieve Relevant Chunks\n",
    "def retrieve_chunks(query: str, retriever: BaseRetriever, query_type: str, years: List[str]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Retrieve document chunks dynamically based on query type.\n",
    "    \"\"\"\n",
    "    if query_type == \"basic\":\n",
    "        if years:\n",
    "            query = f\"{query} {' '.join(years)}\"\n",
    "        return retriever.invoke(query)\n",
    "\n",
    "    elif query_type == \"comparison\":\n",
    "        entities = extract_entities_from_query(query)\n",
    "        chunks = []\n",
    "        for entity in entities:\n",
    "            for year in years:\n",
    "                chunks.extend(retriever.invoke(f\"{entity} {query} {year}\"))\n",
    "        return chunks\n",
    "\n",
    "    elif query_type == \"aggregation\":\n",
    "        if years:\n",
    "            query = f\"{query} {' '.join(years)}\"\n",
    "        return retriever.invoke(query)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported query type.\")\n",
    "    \n",
    "# Step 4: find relevant documents\n",
    "def advanced_retrieve(query: str, retriever: BaseRetriever) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Advanced retriever pipeline for handling complex queries with LLM integration.\n",
    "    \"\"\"\n",
    "    query_type, years = classify_query_with_year(query)\n",
    "    chunks = retrieve_chunks(query, retriever, query_type, years)\n",
    "    return chunks\n",
    "\n",
    "# Step 5: Define Advanced Conversational Retriever   \n",
    "class AdvancedConversationalRetriever(BaseRetriever):\n",
    "    retriever: BaseRetriever = Field(...)  # Define retriever as a field\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Override the retriever's _get_relevant_documents to use advanced logic.\n",
    "        \"\"\"\n",
    "        return advanced_retrieve(query, self.retriever)\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Asynchronous version of _get_relevant_documents.\n",
    "        \"\"\"\n",
    "        return self.retriever.invoke({\"query\": query})  # Use invoke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Retriever\n",
    "def build_retriever(vector_store: FAISS, top_k: int = 15) -> BaseRetriever:\n",
    "    \"\"\"\n",
    "    Build a retriever from the vector store.\n",
    "    \"\"\"\n",
    "    return vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": top_k})\n",
    "\n",
    "retriever = build_retriever(vector_store)\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=5\n",
    ")\n",
    "\n",
    "#Initialize the Chatprompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a helpful assistant. Use the following context to answer the user's question. \"\n",
    "        \"If the answer is not in the context, say 'I don't have an exact answer, can I help with something else?'\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"Context:\\n{context}\\n\\nQuestion:\\n{question}\"\n",
    "    )\n",
    "])\n",
    "# initialize the advanced_retriever\n",
    "advanced_retriever = AdvancedConversationalRetriever(retriever=retriever)\n",
    "# define retrieval chain\n",
    "retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=advanced_retriever,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": chat_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  What were Tesla's profit numbers for 2023\n",
      "Bot: In 2023, Tesla's net income was $14,974 million, and the comprehensive income was $15,192 million.\n",
      "User: compare tesla profit fro 2023 and 2022\n",
      "Bot: In 2023, Tesla's net income attributable to common stockholders was $15.00 billion, which represents a favorable change of $2.44 billion compared to the net income of $12.56 billion in 2022.\n",
      "User: exit\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Final Step: Process User Query\n",
    "chat_history = [] # List to store chat history\n",
    "while True:\n",
    "    query = input(\"Enter your question please: \")\n",
    "    print(\"User:\", query)\n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = retrieval_chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "    print(f\"Bot: {response['answer']}\")\n",
    "\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    chat_history.append(AIMessage(content=response[\"answer\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
